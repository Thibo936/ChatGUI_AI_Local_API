# ChatGUI_AI_Local_API

> **English üá¨üáß | Fran√ßais üá´üá∑**
> Bilingual README for a local AI chat GUI.

---

## English üá¨üáß

### Overview

**ChatGUI_AI_Local_API** is a lightweight desktop application (Python‚ÄØ+‚ÄØPySide6) to chat with:

* **Local models** served by **[Ollama](https://ollama.com/)** (`localhost:11434`)
* **OpenAI models** (if `OPENAI_API_KEY` is set)

It features a clean GUI, multi-conversation, model favourites, visible chain-of-thought, and real-time resource usage.

### Key features

* Multi-conversation sidebar with auto-save to `%APPDATA%/OllamaChats`
* Toggle assistant chain-of-thought (`<think>‚Ä¶</think>`) with a single click
* Model favourites ‚≠ê and instant switch
* Token statistics (total & tok/s) + CPU/RAM monitor
* Automatic checks for missing Python dependencies & VC++ runtime on Windows
* Automatic detection and launch of Ollama server if not running
* OpenAI model support if API key is set

### Prerequisites

| Requirement      | Notes                                                        |
| ---------------- | ------------------------------------------------------------ |
| Python **3.10+** | Windows / macOS / Linux                                      |
| **Ollama**       | Needed only for local models ‚Äì must run on `localhost:11434` |
| `OPENAI_API_KEY` | Optional ‚Äì enables OpenAI models                             |

### Installation

```bash
# 1. Clone
$ git clone https://github.com/your‚Äëname/ChatGUI_AI_Local_API.git
$ cd ChatGUI_AI_Local_API

# 2. Create venv (recommended)
$ python -m venv .venv
$ source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 3. Install dependencies
$ pip install -r requirements.txt
```

### Running

```bash
python main.py
```

On first launch, the app will propose to install missing Python packages, VC++ redistributable, or start **Ollama** if not detected.

### Environment variables

* `OPENAI_API_KEY` ‚Äì OpenAI key (optional)
* `LOCALAPPDATA`  ‚Äì Overrides default data directory on Windows

### File structure

```
main.py                  # Main application
config.py                # Configuration
ollama_client.py         # Ollama API client
chat_window.py           # Main GUI window
models.py                # Message dataclass
utils.py                 # Utilities (logging, checks, etc.)
requirements.txt         # Dependencies
%APPDATA%/OllamaChats/   # Auto-saved chats & settings
  ‚îú‚îÄ‚îÄ model_favorites.json
  ‚îî‚îÄ‚îÄ <uuid>.json        # One file per conversation
```

### Packaging (Windows)

```bash
pip install pyinstaller
pyinstaller --onefile --noconsole main.py
```

### Contributing

Pull requests and issues are welcome!

### License

MIT

---

## Fran√ßais üá´üá∑

### Aper√ßu

**ChatGUI_AI_Local_API** est une application de bureau l√©g√®re (Python‚ÄØ+‚ÄØPySide6) qui permet de discuter‚ÄØ:

* avec des **mod√®les locaux** servis par **[Ollama](https://ollama.com/)** (`localhost:11434`)
* avec des **mod√®les OpenAI** (si la variable `OPENAI_API_KEY` est d√©finie)

Elle propose une interface soign√©e, la gestion de plusieurs conversations, des mod√®les favoris, l‚Äôaffichage des pens√©es de l‚ÄôIA et la surveillance des ressources syst√®me.

### Fonctionnalit√©s cl√©s

* Barre lat√©rale multi-conversations avec sauvegarde automatique dans `%APPDATA%/OllamaChats`
* Affichage/masquage des pens√©es de l‚ÄôIA (`<think>‚Ä¶</think>`) en un clic
* Favoris de mod√®les ‚≠ê et changement instantan√©
* Statistiques de tokens (total & tok/s) + moniteur CPU/RAM en temps r√©el
* V√©rification automatique des d√©pendances Python et du runtime VC++ sous Windows
* D√©tection et lancement automatique du serveur Ollama si n√©cessaire
* Prise en charge des mod√®les OpenAI si la cl√© API est d√©finie

### Pr√©requis

| Pr√©-requis       | Notes                                                                              |
| ---------------- | ---------------------------------------------------------------------------------- |
| Python **3.10+** | Windows / macOS / Linux                                                            |
| **Ollama**       | N√©cessaire uniquement pour les mod√®les locaux ‚Äì doit tourner sur `localhost:11434` |
| `OPENAI_API_KEY` | Optionnel ‚Äì active les mod√®les OpenAI                                              |

### Installation

```bash
# 1. Cloner le d√©p√¥t
$ git clone https://github.com/votre‚Äëpseudo/ChatGUI_AI_Local_API.git
$ cd ChatGUI_AI_Local_API

# 2. Cr√©er un virtualenv (recommand√©)
$ python -m venv .venv
$ source .venv/bin/activate   # Windows : .venv\Scripts\activate

# 3. Installer les d√©pendances
$ pip install -r requirements.txt
```

### Lancement

```bash
python main.py
```

Au premier d√©marrage, l‚Äôapplication propose d‚Äôinstaller les paquets Python manquants, le runtime VC++ ou de lancer **Ollama** s‚Äôil n‚Äôest pas d√©tect√©.

### Variables d‚Äôenvironnement

* `OPENAI_API_KEY` ‚Äì Cl√© OpenAI (optionnel)
* `LOCALAPPDATA`  ‚Äì Red√©finit le r√©pertoire de donn√©es sous Windows

### Arborescence

```
main.py                  # Application principale
config.py                # Configuration
ollama_client.py         # Client API Ollama
chat_window.py           # Fen√™tre principale
models.py                # Dataclass Message
utils.py                 # Utilitaires (log, v√©rifications, etc.)
requirements.txt         # D√©pendances
%APPDATA%/OllamaChats/   # Conversations et param√®tres sauvegard√©s
  ‚îú‚îÄ‚îÄ model_favorites.json
  ‚îî‚îÄ‚îÄ <uuid>.json        # Une conversation par fichier
```

### Cr√©ation d‚Äôun ex√©cutable Windows

```bash
pip install pyinstaller
pyinstaller --onefile --noconsole main.py
```

### Contribuer

Les pull requests et issues sont les bienvenus‚ÄØ!

### Licence

MIT
